{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77aaa291",
   "metadata": {},
   "source": [
    "# Nwobbi Olisa - 100893871\n",
    "# Olomola Adekunle - 100887419\n",
    "\n",
    "# Consulting and Professional Communication Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d0ef9",
   "metadata": {},
   "source": [
    "### Using HuggingFace API and Gradio.io to create an interactive application for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa938338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection']\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "\n",
    "#Get the list of tasks that are supported by Huggingface pipeline\n",
    "print(PIPELINE_REGISTRY.get_supported_tasks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab536d",
   "metadata": {},
   "source": [
    "### We will be using the *Text Generation* task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c857e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Model for Visual-question-answering: \n",
      "{'model': {'pt': ('gpt2', '6c0e608'), 'tf': ('gpt2', '6c0e608')}}\n"
     ]
    }
   ],
   "source": [
    "# Get information about a specific task\n",
    "\n",
    "print(\"Default Model for Text Generation: \")\n",
    "print(PIPELINE_REGISTRY.check_task('text-generation')[1].get('default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a00e7",
   "metadata": {},
   "source": [
    "### Loading the pipeline and testing the default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793f482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy was entering from the middle of the mall, so I asked him if something happened there. He said, \"Yes, I saw her coming out of her car, but now she wasn't wearing that helmet. She was just carrying a bag to do so, and it was a metal detector. And she was going with the boy until she had to come back to her car. I told the police that after that time, she got out her bag after the boy had left.\"\n",
      "\n",
      "In any event, he said he has had plenty of contact with the girl and her friends this week. He said he's not sure she will want to be involved again, but said that she's still very young and that she wants to continue learning\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The boy was entering\"\n",
    "text = generator(prompt, max_length=150, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858127f5",
   "metadata": {},
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe89de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man was jumping through the air with her as the woman tried desperately to push him down. Corinth said this was the first time she had ever seen him jump before.\n",
      "\n",
      "\"I looked on, and then I realized,\" Corinth said.\n",
      "\n",
      "It seems like so many things have happened in this movie, that it makes sense that he's been trying to get down to the level of jumping, but Corinth doesn't see it as a threat. In fact, he says, he's only a kid who's been making an early impression on all of his teachers.\n",
      "\n",
      "\"My first lesson was 'I want my kid to jump high,'\" he says. \"So my first lesson was 'I know you can jump, but I still get this thrill from jumping. This kid is going to do it.'\"\n",
      "\n",
      "He knows there's just so much going on, and that there will be plenty of people who know how to get down at a specific angle to get him down if he decides to do it the right way. Corinth said he didn't have a real clue what was going on between now and now when he hit that first jump.\n",
      "\n",
      "He's a fan of the game of hockey, and he and his kids like the opportunity to see it firsthand, and he tries to keep his mind on how it's played out.\n",
      "\n",
      "\"I want to help change the game,\" he says, \"so I want it to be different. Because you try to be different, there's a huge disconnect â€” there's this thing when your baby cries and it makes you think, 'I need to jump.'\n",
      "\n",
      "\"When you see that movie, you think, Oh my God, he can do it a long time\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Define a topic-related prompt\n",
    "topic_prompt = \"The man was jumping\"\n",
    "\n",
    "# Generate text related to the topic\n",
    "text = generator(topic_prompt, max_length=350, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb044e8",
   "metadata": {},
   "source": [
    "### Generate multiple text variations using more than one prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f81fd02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The boy was entering\n",
      "Generated Text: The boy was entering the school from an elementary student's back when he heard another girl screaming. He immediately ran. Not many other parents reported an incident like this.\n",
      "\n",
      "\"These kids are not going to do this. They have no chance. These boys will do whatever they want because of this bullying,\" another neighbor said.\n",
      "\n",
      "Police say the five men were arrested on suspicion of criminal trespass. They will be charged in November.\n",
      "\n",
      "Copyright Associated Press / NBC 10 Philadelphia\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time\n",
      "Generated Text: Once upon a time. And I am sayingushima. And there's no other alternative.\n",
      "\n",
      "Yatagarasu: But you don't just say, \"No problem. You just wanted to see me on camera.\"\n",
      "\n",
      "Satsuki: Yes! However, it's true! That was the real world! I was born on a big island in Japan without my parents even knowing. You were born in an ordinary day! Even though I've been here on the island for a while, every time I walked a step, I would suddenly feel something different. Like it is for something you have to get used to. No matter what, even if the person you are talking to feels bad for you, it feels good to accept this!\n",
      "\n",
      "Prompt: In a galaxy far away\n",
      "Generated Text: In a galaxy far away from a star system, such as this particular star, the Milky Way's light has drifted far enough to become visible.\n",
      "\n",
      "Such dark matter is one of our most elusive objects. In our galactic neighbourhood, scientists believe this phenomenon could possibly have been the basis for the birth of this planet, which was about 4.6 billion years ago in the earliest stages of the universe.\n",
      "\n",
      "The most promising explanation is that, once a massive shock to Earth was detected between Earth and the outer solar system, Brewers started moving the particles to the surface with a beam of energy.\n",
      "\n",
      "Now, however, they have seen a bright, moving object that moves faster than the speed of light in nearly all directions. That object is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "prompts = [\"The boy was entering\", \"Once upon a time\", \"In a galaxy far away\"]\n",
    "for prompt in prompts:\n",
    "    text = generator(prompt, max_length=150, num_return_sequences=1)[0]['generated_text']\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b1088",
   "metadata": {},
   "source": [
    "### Creating the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50dfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:16: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Textbox(label=\"Enter a prompt\"),\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:16: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Textbox(label=\"Enter a prompt\"),\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:16: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Textbox(label=\"Enter a prompt\"),\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:17: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Number(default=50, label=\"Word Limit\")\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:17: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Number(default=50, label=\"Word Limit\")\n",
      "C:\\Users\\Olisa\\AppData\\Local\\Temp\\ipykernel_20844\\525322422.py:19: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://cb9c436c5d9a9f2487.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cb9c436c5d9a9f2487.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "def generate_text(prompt, word_limit):\n",
    "    generated_text = generator(prompt, max_length=150, num_return_sequences=1)[0]['generated_text']\n",
    "    generated_words = generated_text.split()[:int(word_limit)]\n",
    "    generated_text = \" \".join(generated_words)\n",
    "    return generated_text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\n",
    "        gr.inputs.Textbox(label=\"Enter a prompt\"),\n",
    "        gr.inputs.Number(default=50, label=\"Word Limit\")\n",
    "    ],\n",
    "    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
    "    title=\"Text Generation App\",\n",
    "    description=\"Generate text based on prompts with word limit.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716af954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
